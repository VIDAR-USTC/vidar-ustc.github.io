---
title: "Research Topic: Super-Resolution"
excerpt: "Super-Resolution"
author_profile: True
permalink: /super-resolution/2024-New.html
---

**[▼ 2024 New](/super-resolution/2024-New){: style="color: rgb(191, 0, 0)"}**
**[▶ Image](/super-resolution/sr-image){: style="color: rgb(191, 0, 0)"}**
**[▶ Video](/super-resolution/sr-video){: style="color: rgb(191, 0, 0)"}**
**[▶ Spectrum](/super-resolution/sr-spectrum){: style="color: rgb(191, 0, 0)"}**
**[▶ Light Field](/super-resolution/sr-light-field){: style="color: rgb(191, 0, 0)"}**

<!-- [Others](/super-resolution/sr-other) -->

## 2024 New

**Continuous Spatial-Spectral Reconstruction via Implicit Neural Representation** <br>
_Ruikang Xu, Mingde Yao, Chang Chen, Lizhi Wang, Zhiwei Xiong_ <br>
<span><pub>International Journal of Computer Vision (IJCV), 2024</pub></span> <br>
<span style="color: green;"> Coming soon! </span>

**Toward DNN of LUTs: Learning Efficient Image Restoration with Multiple Look-Up Tables** <br>
_Jiacheng Li, Chang Chen, Zhen Cheng, Zhiwei Xiong_ <br>
<span><pub>IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 2024</pub></span> <br>
[Paper](https://ieeexplore.ieee.org/document/10530442/){:target="\_blank"} |
[Code](https://github.com/ddlee-cn/MuLUT){:target="\_blank"} |
<a onclick='expandABS("jiacheng_pami24")'> Abstract </a>

<div style="display: none;" class=abs id="jiacheng_pami24"><br>
The widespread usage of high-definition screens on edge devices stimulates a strong demand for efficient image restoration algorithms. The way of caching deep learning models in a look-up table (LUT) is recently introduced to respond to this demand. However, the size of a single LUT grows exponentially with the increase of its indexing capacity, which restricts its receptive field and thus the performance. To overcome this intrinsic limitation of the single-LUT solution, we propose a universal method to construct multiple LUTs like a neural network, termed MuLUT. Firstly, we devise novel complementary indexing patterns, as well as a general implementation for arbitrary patterns, to construct multiple LUTs in parallel. Secondly, we propose a re-indexing mechanism to enable hierarchical indexing between cascaded LUTs. Finally, we introduce channel indexing to allow cross-channel interaction, enabling LUTs to process color channels jointly. In these principled ways, the total size of MuLUT is linear to its indexing capacity, yielding a practical solution to obtain superior performance with the enlarged receptive field. We examine the advantage of MuLUT on various image restoration tasks, including super-resolution, demosaicing, denoising, and deblocking. MuLUT achieves a significant improvement over the single-LUT solution, e.g., up to 1.1dB PSNR for super-resolution and up to 2.8dB PSNR for grayscale denoising, while preserving its efficiency, which is 100× less in energy cost compared with lightweight deep neural networks. Our code and trained models are publicly available at https://github.com/ddlee-cn/MuLUT.

</div>

**Confidence-Based Iterative Generation for Real-World Image Super-Resolution** <br>
_Jialun Peng, Xin Luo, Jingjing Fu, Dong Liu_ <br>
<span><pub>European Conference on Computer Vision (ECCV), 2024</pub></span> <br>
<span style="color: green;"> Coming soon! </span>

**Look-Up Table Compression for Efficient Image Restoration** <br>
_Yinglong Li, Jiacheng Li, Zhiwei Xiong_ <br>
<span><pub>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024 <highlighted>(Highlight)</highlighted> </pub></span> <br>
[Paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Look-Up_Table_Compression_for_Efficient_Image_Restoration_CVPR_2024_paper.pdf){:target="\_blank"} |
[Code](https://github.com/leenas233/DFC){:target="\_blank"} |
<a onclick='expandABS("yinglong_cvpr24")'> Abstract </a>

<div style="display: none;" class=abs id="yinglong_cvpr24"><br>
Look-Up Table (LUT) has recently gained increasing attention for restoring High-Quality (HQ) images from Low-Quality (LQ) observations thanks to its high computational efficiency achieved through a" space for time" strategy of caching learned LQ-HQ pairs. However incorporating multiple LUTs for improved performance comes at the cost of a rapidly growing storage size which is ultimately restricted by the allocatable on-device cache size. In this work we propose a novel LUT compression framework to achieve a better trade-off between storage size and performance for LUT-based image restoration models. Based on the observation that most cached LQ image patches are distributed along the diagonal of a LUT we devise a Diagonal-First Compression (DFC) framework where diagonal LQ-HQ pairs are preserved and carefully re-indexed to maintain the representation capacity while non-diagonal pairs are aggressively subsampled to save storage. Extensive experiments on representative image restoration tasks demonstrate that our DFC framework significantly reduces the storage size of LUT-based models (including our new design) while maintaining their performance. For instance DFC saves up to 90% of storage at a negligible performance drop for x4 super-resolution. The source code is available on GitHub: https://github.com/leenas233/DFC.

</div>

---

**[▲ 2024 New](/super-resolution/2024-New){: style="color: rgb(191, 0, 0)"}**
**[▶ Image](/super-resolution/sr-image){: style="color: rgb(191, 0, 0)"}**
**[▶ Video](/super-resolution/sr-video){: style="color: rgb(191, 0, 0)"}**
**[▶ Spectrum](/super-resolution/sr-spectrum){: style="color: rgb(191, 0, 0)"}**
**[▶ Light Field](/super-resolution/sr-light-field){: style="color: rgb(191, 0, 0)"}**

<!-- **UDNet: Up-Down Network for Compact and Efficient Feature Representation in Image Super-Resolution** <br>
*Chang Chen, Xinmei Tian, Feng Wu, Zhiwei Xiong* <br>
<span><pub>IEEE International Conference on Computer Vision Workshops (ICCVW), 2017</pub></span> <br>
[Paper](https://ieeexplore.ieee.org/document/8265339){:target="_blank"} |
<a onclick='expandABS("chen17")'> Abstract </a>
<div style="display: none;" class=abs id="chen17"><br>
Recently, image super-resolution (SR) using convolutional neural networks (CNNs) have achieved remarkable performance. However, there is a tradeoff between performance and speed of SR, depending on whether feature representation and learning are conducted in high-resolution (HR) or low-resolution (LR) space. Generally, to pursue real-time SR, the number of parameters in CNNs has to be restricted, which results in performance degradation. In this paper, we propose a compact and efficient feature representation for real-time SR, named up-down network (UDNet). Specifically, a novel hourglass-shape structure is introduced by combining transposed convolution and spatial aggregation. This structure enables the network to transfer the feature representations between LR and HR spaces multiple times to learn a better mapping. Comprehensive experiments demonstrate that, compared with existing CNN models, UDNet achieves real-time SR without performance degradation on widely used benchmarks.
</div>
 -->

<!--
**Example-Based Super-Resolution With Soft Information and Decision** <br>
*Zhiwei Xiong, Dong Xu, Xiaoyan Sun, Feng Wu* <br>
<span><pub>IEEE Transactions on Multimedia (T-MM), 2013</pub></span> <br>
[Paper](http://ieeexplore.ieee.org/document/6518133/){:target="_blank"} |
<a onclick='expandABS("xiong13")'> Abstract </a>
<div style="display: none;" class=abs id="xiong13"><br>
The one-to-one correspondence between co-occurrence image patches of two different resolutions is extensively used in example-based super-resolution (SR). Due to the dimensionality gap between low resolution (LR) and high resolution (HR) spaces, however, an LR patch may correspond to a number of HR patches in practice. This ambiguity is difficult to be overcome with examples representing a deterministic mapping. In this paper, we propose a statistical method for exploiting the one-to-many correspondence between LR and HR patches, which we call soft information and decision. Soft information means an LR patch is mapped to a pixel-wise distribution of all its possible HR counterparts, rather than a single or a limited set of HR candidates. Relying on the soft information, example-based SR is then regarded as an optimization problem to best preserve the local consistency in the recovered HR image. This problem is solved with an efficient message passing algorithm with a factor graph model. The final decision on the HR pixel value is made upon the maximum a posteriori estimation and is called a soft decision. Experimental results demonstrate the superiority of the proposed method compared with the state-of-the-art methods, in terms of both the subjective and objective quality of synthesized HR images.
</div>


**Robust Web Image/Video Super-Resolution** <br>
*Zhiwei Xiong, Xiaoyan Sun, Feng Wu* <br>
<span><pub>IEEE Transactions on Image Processing (T-IP), 2010</pub></span> <br>
[Paper](https://ieeexplore.ieee.org/abstract/document/5430911/){:target="_blank"} |
<a onclick='expandABS("xiong10")'> Abstract </a>
<div style="display: none;" class=abs id="xiong10"><br>
This paper proposes a robust single-image super-resolution method for enlarging low quality web image/video degraded by downsampling and compression. To simultaneously improve the resolution and perceptual quality of such web image/video, we bring forward a practical solution which combines adaptive regularization and learning-based super-resolution. The contribution of this work is twofold. First, we propose to analyze the image energy change characteristics during the iterative regularization process, i.e., the energy change ratio between primitive (e.g., edges, ridges and corners) and nonprimitive fields. Based on the revealed convergence property of the energy change ratio, appropriate regularization strength can then be determined to well balance compression artifacts removal and primitive components preservation. Second, we verify that this adaptive regularization can steadily and greatly improve the pair matching accuracy in learning-based super-resolution. Consequently, their combination effectively eliminates the quantization noise and meanwhile faithfully compensates the missing high-frequency details, yielding robust super-resolution performance in the compression scenario. Experimental results demonstrate that our solution produces visually pleasing enlargements for various web images/videos.
</div>


**Image Hallucination with Feature Enhancement** <br>
*Zhiwei Xiong, Xiaoyan Sun, Feng Wu* <br>
<span><pub>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009</pub></span> <br>
[Paper](https://ieeexplore.ieee.org/abstract/document/5206630/){:target="_blank"} |
<a onclick='expandABS("xiong09")'> Abstract </a>
<div style="display: none;" class=abs id="xiong09"><br>
Example-based super-resolution recovers missing high frequencies in a magnified image by learning the correspondence between co-occurrence examples at two different resolution levels. As high-resolution examples usually contain more details and are of higher dimensionality in comparison with low-resolution ones, the mapping from low-resolution to high-resolution is an ill-posed problem. Rather than imposing more complicated mapping constraints, we propose to improve the mapping accuracy by enhancing low-resolution examples in terms of mapped features, e.g., derivatives and primitives. A feature enhancement method is presented through a combination of interpolation with prefiltering and non-blind sparse prior deblurring. By enhancing low-resolution examples, unique feature information carried by high-resolution examples is decreased. This regularization reduces the intrinsic dimensionality disparity between two different resolution examples and thus improves the feature mapping accuracy. Experiments demonstrate our super-resolution scheme with feature enhancement produces high quality results both perceptually and quantitatively.

</div> -->
