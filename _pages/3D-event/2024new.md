---
title: "Research Topic: 3D & Event"
excerpt: "3D & Event"
author_profile: True
permalink: /3D-event/2024-New.html
---

**[▼ 2024 New](/3D-event/2024-New){: style="color: rgb(191, 0, 0)"}**
**[▶ Active 3D](/3D-event/active3d){: style="color: rgb(191, 0, 0)"}**
**[▶ Passive 3D](/3D-event/passive3d){: style="color: rgb(191, 0, 0)"}**
**[▶ Event](/3D-event/event){: style="color: rgb(191, 0, 0)"}**
**[▶ Point Cloud](/3D-event/point-cloud){: style="color: rgb(191, 0, 0)"}**

## 2024 New

**Exploiting Dual-Correlation for Multi-frame Time-of-Flight Denoising** <br>
_Guanting Dong, Yueyi Zhang, Xiaoyan Sun, Zhiwei Xiong_ <br>
<span><pub>European Conference on Computer Vision (ECCV), 2024</pub></span> <br>
<span style="color: green;">Coming soon! </span>

**Event-Adapted Video Super-Resolution** <br>
_Zeyu Xiao, Dachun Kai, Yueyi Zhang, Zheng-Jun Zha, Xiaoyan Sun, Zhiwei Xiong_ <br>
<span><pub>European Conference on Computer Vision (ECCV), 2024</pub></span> <br>
<span style="color: green;">Coming soon! </span>

**Joint Flow Estimation from Point Clouds and Event** <br>
_Hanlin Li, Yueyi Zhang, Guanting Dong, Shida Sun, Zhiwei Xiong_ <br>
<span><pub>IEEE International Conference on Multimedia and Expo (ICME), 2024</pub></span> <br>
<span style="color: green;">Coming soon! </span>

**Depth From Asymmetric Frame-Event Stereo: A Divide-and-Conquer Approach** <br>
_Xihao Chen, Wenming Weng, Yueyi Zhang, Zhiwei Xiong_ <br>
<span><pub>IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2024</pub></span> <br>
[Paper](https://openaccess.thecvf.com/content/WACV2024/html/Chen_Depth_From_Asymmetric_Frame-Event_Stereo_A_Divide-and-Conquer_Approach_WACV_2024_paper.html){:target="\_blank"} |
[Code](https://github.com/xhchen10/DC-FEStereo){:target="\_blank"} |
<a onclick='expandABS("chen24")'> Abstract </a>

<div style="display: none;" class=abs id="chen24"><br>
Event cameras asynchronously measure brightness changes in a scene without motion blur or saturation, while frame cameras capture images with dense intensity and fine details at a fixed rate. The exclusive advantages of the two modalities make depth estimation from Stereo Asymmetric Frame-Event (SAFE) systems appealing. However, due to the inevitable information absence of one modality in certain challenging regions, existing stereo matching methods lose efficacy for asymmetric inputs from SAFE systems. In this paper, we propose a divide-and-conquer approach that decomposes depth estimation from SAFE systems into three sub-tasks, i.e., frame-event stereo matching, frame-based Structure-from-Motion (SfM), and event-based SfM. In this way, the above challenging regions are addressed by monocular SfM, which estimates robust depth with two views belonging to the same functioning modality. Moreover, we propose a dual sampling strategy to construct cost volumes with identical spatial locations and depth hypotheses for different sub-tasks, which enables sub-task fusion at the cost volume level. To tackle the occlusion issue raised by the sampling strategy, we further introduce a temporal fusion scheme to utilize long-term sequential inputs with multi-view information. Experimental results validate the superior performance of our method over existing solutions.
</div>

<!-- **Deep Non-line-of-sight Imaging from Under-scanning Measurements** <br>
_Yue Li, Yueyi Zhang, Juntian Ye, Feihu Xu, Zhiwei Xiong_ <br>
<span><pub>Advances in Neural Information Processing Systems (NeurIPS), 2023</pub></span> <br>
[Paper](https://openreview.net/forum?id=JCN9YsZiwB){:target="\_blank"} |
[Code](https://github.com/Depth2World/Under-scanning_NLOS){:target="\_blank"} |
<a onclick='expandABS("li23nips")'> Abstract </a>

<div style="display: none;" class=abs id="li23nips"><br>
Active confocal non-line-of-sight (NLOS) imaging has successfully enabled seeing around corners relying on high-quality transient measurements. However, acquiring spatial-dense transient measurement is time-consuming, raising the question of how to reconstruct satisfactory results from under-scanning measurements (USM). The existing solutions, involving the traditional algorithms, however, are hindered by unsatisfactory results or long computing times. To this end, we propose the first deep-learning-based approach to NLOS imaging from USM. Our proposed end-to-end network is composed of two main components: the transient recovery network (TRN) and the volume reconstruction network (VRN). Specifically, TRN takes the under-scanning measurements as input, utilizes a multiple kernel feature extraction module and a multiple feature fusion module, and outputs sufficient-scanning measurements at the high-spatial resolution. Afterwards, VRN incorporates the linear physics prior of the light-path transport model and reconstructs the hidden volume representation. Besides, we introduce regularized constraints that enhance the perception of more local details while suppressing smoothing effects. The proposed method achieves superior performance on both synthetic data and public real-world data, as demonstrated by extensive experimental results with different under-scanning grids. Moreover, the proposed method delivers impressive robustness at an extremely low scanning grid (i.e., 8x8) and offers high-speed inference (i.e., 50 times faster than the existing iterative solution).

</div> -->

---

**[▲ 2024 New](/3D-event/2024-New){: style="color: rgb(191, 0, 0)"}**
**[▶ Active 3D](/3D-event/active3d){: style="color: rgb(191, 0, 0)"}**
**[▶ Passive 3D](/3D-event/passive3d){: style="color: rgb(191, 0, 0)"}**
**[▶ Event](/3D-event/event){: style="color: rgb(191, 0, 0)"}**
**[▶ Point Cloud](/3D-event/point-cloud){: style="color: rgb(191, 0, 0)"}**
