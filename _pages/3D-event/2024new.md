---
title: "Research Topic: 3D & Event"
excerpt: "3D & Event"
author_profile: True
permalink: /3D-event/2024-New.html
---

**[▼ 2024 New](/3D-event/2024-New){: style="color: rgb(191, 0, 0)"}**
**[▶ Active 3D](/3D-event/active3d){: style="color: rgb(191, 0, 0)"}**
**[▶ Passive 3D](/3D-event/passive3d){: style="color: rgb(191, 0, 0)"}**
**[▶ Event](/3D-event/event){: style="color: rgb(191, 0, 0)"}**
**[▶ Point Cloud](/3D-event/point-cloud){: style="color: rgb(191, 0, 0)"}**

## 2024 New

**Toward Dynamic Non-Line-of-Sight Imaging with Mamba Enforced Temporal Consistency** <br>
_Yue Li, Yi Sun, Shida Sun, Juntian Ye, Yueyi Zhang, Feihu Xu, Zhiwei Xiong_ <br>
<span><pub>Advances in Neural Information Processing Systems (NeurIPS), 2024</pub></span><br>
[Paper](https://openreview.net/pdf/8ceb8c4575a29d6db10203bc67a5f763ebcf0ef7.pdf){:target="\_blank"} |
[Code](https://github.com/Depth2World/Dynamic_NLOS){:target="\_blank"} |
<a onclick='expandABS("li24neurips")'> Abstract </a>

<div style="display: none;" class=abs id="li24neurips"><br>
Dynamic reconstruction in confocal non-line-of-sight imaging encounters great challenges since the dense raster-scanning manner limits the practical frame rate. A fewer pioneer works reconstruct high-resolution volumes from the under-scanning transient measurements but overlook temporal consistency among transient frames. To fully exploit multi-frame information, we propose the first spatial-temporal Mamba (ST-Mamba) based method tailored for dynamic reconstruction of transient videos. Our method capitalizes on neighbouring transient frames to aggregate the target 3D hidden volume. Specifically, the interleaved features extracted from the input transient frames are fed to the proposed ST-Mamba blocks, which leverage the time-resolving causality in transient measurement. The cross STMamba blocks are then devised to integrate the adjacent transient features. The target high-resolution transient frame is subsequently recovered by the transient spreading module. After transient fusion and recovery, a physical-based network is employed to reconstruct the hidden volume. To tackle the substantial noise inherent in transient videos, we propose a wave-based loss function to impose constraints within the phasor field. Besides, we introduce a new dataset, comprising synthetic videos for training and real-world videos for evaluation. Extensive experiments showcase the superior performance of our method on both synthetic data and real-world data captured by different imaging setups. The code and data are available at https://github.com/Depth2World/Dynamic_NLOS.
</div>

**Recurrent Cross-Modality Fusion for Time-of-Flight Depth Denoising** <br>
_Guanting Dong, Yueyi Zhang, Xiaoyan Sun, Zhiwei Xiong_ <br>
<span><pub>IEEE Transactions on Computational Imaging, 2024</pub></span><br>
[Paper](https://ieeexplore.ieee.org/document/10750330){:target="\_blank"} |
[Code](https://github.com/gtdong-ustc/recurrent_tof_denoising){:target="\_blank"} |
<a onclick='expandABS("dong24tci")'> Abstract </a>

<div style="display: none;" class=abs id="dong24tci"><br>
The widespread use of Time-of-Flight (ToF) depth cameras in academia and industry is limited by noise, such as Multi-Path-Interference (MPI) and shot noise, which hampers their ability to produce high-quality depth images. Learning-based ToF denoising methods currently in existence often face challenges in delivering satisfactory performance in complex scenes. This is primarily attributed to the impact of multiple reflected signals on the formation of MPI, rendering it challenging to predict MPI directly through spatially-varying convolutions. To address this limitation, we adopt a recurrent architecture that exploits the prior that MPI is decomposable into an additive combination of the geometric information for the neighboring pixels. Our approach employs a Gated Recurrent Unit (GRU) based network to estimate a long-distance aggregation process, simplifying the MPI removal and updating depth correction over multiple steps. Additionally, we introduce a global restoration module and a local update module to fuse depth and amplitude features, which improves denoising performance and prevents structural distortions. Experimental results on both synthetic and real-world datasets demonstrate the superiority of our approach over state-of-the-art methods.
</div>

**Exploiting Dual-Correlation for Multi-frame Time-of-Flight Denoising** <br>
_Guanting Dong, Yueyi Zhang, Xiaoyan Sun, Zhiwei Xiong_ <br>
<span><pub>European Conference on Computer Vision (ECCV), 2024</pub></span> <br>
[Paper](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03302.pdf){:target="\_blank"} |
[Code](#){:target="\_blank"} |
<a onclick='expandABS("dong24eccv")'> Abstract </a>

<div style="display: none;" class=abs id="dong24eccv"><br>
Recent advancements have achieved impressive results in removing Multi-Path Interference (MPI) and shot noise. However, these methods only utilize a single frame of ToF data, neglecting the correlation between frames. The multi-frame ToF denoising is still underexplored. In this paper, we propose the first learning-based framework for multi-frame ToF denoising. Different from previous frameworks, ours leverages the correlation between inter frames to guide the ToF noise removal with a confidence map. Specifically, we introduce a Dual-Correlation Estimation Module, which exploits both intra- and inter-correlation. The intra-correlation explicitly establishes the relevance between the spatial positions of geometric objects within the scene, aiding in depth residual initialization. The inter-correlation discerns variations in ToF noise distribution across different frames, thereby locating the areas with strong noise. To further leverage dual-correlation, we introduce a Confidence-guided Residual Regression Module to predict a confidence map, which guides the residual regression to prioritize the regions with strong ToF noise. The experimental evaluations have consistently shown that our approach outperforms other ToF denoising methods, highlighting its superior performance in effectively reducing strong ToF noise.
</div>

**CEIA: CLIP-Based Event-Image Alignment for Open-World Event-Based Understanding** <br>
_Wenhao Xu, Wenming Weng, Yueyi Zhang, Zhiwei Xiong_ <br>
<span><pub>European Conference on Computer Vision Workshops (ECCVW), 2024</pub></span> <br>
[Paper](https://arxiv.org/pdf/2407.06611.pdf){:target="\_blank"} |
[Code](#){:target="\_blank"} |
<a onclick='expandABS("xu24eccvw")'> Abstract </a>

<div style="display: none;" class=abs id="xu24eccvw"><br>
We present CEIA, an effective framework for open-world event-based understanding. Currently training a large event-text model still poses a huge challenge due to the shortage of paired event-text data. In response to this challenge, CEIA learns to align event and image data as an alternative instead of directly aligning event and text data. Specifically, we leverage the rich event-image datasets to learn an event embedding space aligned with the image space of CLIP through contrastive learning. In this way, event and text data are naturally aligned via using image data as a bridge. Particularly, CEIA offers two distinct advantages. First, it allows us to take full advantage of the existing event-image datasets to make up the shortage of large-scale event-text datasets. Second, leveraging more training data, it also exhibits the flexibility to boost performance, ensuring scalable capability. In highlighting the versatility of our framework, we make extensive evaluations through a diverse range of event-based multi-modal applications, such as object recognition, event-image retrieval, event-text retrieval, and domain adaptation. The outcomes demonstrate CEIA's distinct zero-shot superiority over existing methods on these applications.
</div>

**Event-Adapted Video Super-Resolution** <br>
_Zeyu Xiao, Dachun Kai, Yueyi Zhang, Zheng-Jun Zha, Xiaoyan Sun, Zhiwei Xiong_ <br>
<span><pub>European Conference on Computer Vision (ECCV), 2024</pub></span> <br>
[Paper](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05857.pdf){:target="\_blank"} |
[Code](#){:target="\_blank"} |
<a onclick='expandABS("xiao24eccv")'> Abstract </a>

<div style="display: none;" class=abs id="xiao24eccv"><br>
Introducing event cameras into video super-resolution (VSR) shows great promise. In practice, however, integrating event data as a new modality necessitates a laborious model architecture design. This not only consumes substantial time and effort but also disregards valuable insights from successful existing VSR models. Furthermore, the resource-intensive process of retraining these newly designed structures exacerbates the challenge. In this paper, inspired by recent success of parameter-efficient tuning in reducing the number of trainable parameters of a pre-trained model for downstream tasks, we introduce the Event AdapTER (EATER) for VSR. EATER efficiently utilizes pre-trained VSR model knowledge at the feature level through two lightweight and trainable components: the event-adapted alignment (EAA) unit and the event-adapted fusion (EAF) unit. The EAA unit aligns multiple frames based on the event stream in a coarse-to-fine manner, while the EAF unit efficiently fuses frames with the event stream through a multi-scaled design. Thanks to both units, EATER outperforms the full fine-tuning paradigm. Comprehensive experiments demonstrate the effectiveness of EATER, achieving superior results with parameter efficiency.
</div>

**Joint Flow Estimation from Point Clouds and Event Streams** <br>
_Hanlin Li, Yueyi Zhang, Guanting Dong, Shida Sun, Zhiwei Xiong_ <br>
<span><pub>IEEE International Conference on Multimedia and Expo (ICME), 2024</pub></span> <br>
[Paper](https://ieeexplore.ieee.org/document/10687963){:target="\_blank"} |
[Code](#){:target="\_blank"} |
<a onclick='expandABS("li24icme")'> Abstract </a>

<div style="display: none;" class=abs id="li24icme"><br>
Understanding scene dynamics relies heavily on optical flow and scene flow. Most existing flow estimation methods use low-rate RGB images and point clouds, and match the frames geometrically. However, this approach faces challenges in real-world scenes with intricate motion, occlusion, and noise. To tackle this problem, we combine point clouds with events, which introduce dynamic inter-frame information. We propose a bi-stream neural network that jointly estimates optical flow and scene flow. The event branch extracts dynamic information and estimates optical flow, while the point branch captures scene structure and estimate scene flow. A Spatio-temporal Fusion Block is introduced to fuse the complementary information from points and events. Additionally, we adopt a result-level fusion strategy for direct refinement between the flow predictions of the two branches. We evaluate our model on the real-world datasets DSEC and MVSEC. The experimental results demonstrate superior performance compared to existing methods.
</div>

**Depth From Asymmetric Frame-Event Stereo: A Divide-and-Conquer Approach** <br>
_Xihao Chen, Wenming Weng, Yueyi Zhang, Zhiwei Xiong_ <br>
<span><pub>IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2024</pub></span> <br>
[Paper](https://openaccess.thecvf.com/content/WACV2024/html/Chen_Depth_From_Asymmetric_Frame-Event_Stereo_A_Divide-and-Conquer_Approach_WACV_2024_paper.html){:target="\_blank"} |
[Code](https://github.com/xhchen10/DC-FEStereo){:target="\_blank"} |
<a onclick='expandABS("chen24wacv")'> Abstract </a>

<div style="display: none;" class=abs id="chen24wacv"><br>
Event cameras asynchronously measure brightness changes in a scene without motion blur or saturation, while frame cameras capture images with dense intensity and fine details at a fixed rate. The exclusive advantages of the two modalities make depth estimation from Stereo Asymmetric Frame-Event (SAFE) systems appealing. However, due to the inevitable information absence of one modality in certain challenging regions, existing stereo matching methods lose efficacy for asymmetric inputs from SAFE systems. In this paper, we propose a divide-and-conquer approach that decomposes depth estimation from SAFE systems into three sub-tasks, i.e., frame-event stereo matching, frame-based Structure-from-Motion (SfM), and event-based SfM. In this way, the above challenging regions are addressed by monocular SfM, which estimates robust depth with two views belonging to the same functioning modality. Moreover, we propose a dual sampling strategy to construct cost volumes with identical spatial locations and depth hypotheses for different sub-tasks, which enables sub-task fusion at the cost volume level. To tackle the occlusion issue raised by the sampling strategy, we further introduce a temporal fusion scheme to utilize long-term sequential inputs with multi-view information. Experimental results validate the superior performance of our method over existing solutions.
</div>

---

**[▲ 2024 New](/3D-event/2024-New){: style="color: rgb(191, 0, 0)"}**
**[▶ Active 3D](/3D-event/active3d){: style="color: rgb(191, 0, 0)"}**
**[▶ Passive 3D](/3D-event/passive3d){: style="color: rgb(191, 0, 0)"}**
**[▶ Event](/3D-event/event){: style="color: rgb(191, 0, 0)"}**
**[▶ Point Cloud](/3D-event/point-cloud){: style="color: rgb(191, 0, 0)"}**

<!-- ---
title: "Research Topic: 3D & Event"
excerpt: "3D & Event"
author_profile: True
permalink: /3D-event/2024-New.html
---

**[▼ 2024 New](/3D-event/2024-New){: style="color: rgb(191, 0, 0)"}**
**[▶ Active 3D](/3D-event/active3d){: style="color: rgb(191, 0, 0)"}**
**[▶ Passive 3D](/3D-event/passive3d){: style="color: rgb(191, 0, 0)"}**
**[▶ Event](/3D-event/event){: style="color: rgb(191, 0, 0)"}**
**[▶ Point Cloud](/3D-event/point-cloud){: style="color: rgb(191, 0, 0)"}**

## 2024 New

**Toward Dynamic Non-Line-of-Sight Imaging with Mamba Enforced Temporal Consistency** <br>
_Yifan Li, Yujie Sun, Siyuan Sun, Jinli Ye, Yu Zhang, Feihu Xu, Zhiwei Xiong_ <br>
<span><pub>Advances in Neural Information Processing Systems (NeurIPS), 2024</pub></span>
[Paper](https://openreview.net/pdf/8ceb8c4575a29d6db10203bc67a5f763ebcf0ef7.pdf){:target="\_blank"} |
[Code](https://github.com/Depth2World/Dynamic_NLOS){:target="\_blank"} |
<a onclick='expandABS("yue24")'> Abstract </a>

<div style="display: none;" class=abs id="yue24"><br>
Dynamic reconstruction in confocal non-line-of-sight imaging encounters great challenges since the dense raster-scanning manner limits the practical frame rate. A fewer pioneer works reconstruct high-resolution volumes from the under-scanning transient measurements but overlook temporal consistency among transient frames. To fully exploit multi-frame information, we propose the first spatial-temporal Mamba (ST-Mamba) based method tailored for dynamic reconstruction of transient videos. Our method capitalizes on neighbouring transient frames to aggregate the target 3D hidden volume. Specifically, the interleaved features extracted from the input transient frames are fed to the proposed ST-Mamba blocks, which leverage the time-resolving causality in transient measurement. The cross STMamba blocks are then devised to integrate the adjacent transient features. The target high-resolution transient frame is subsequently recovered by the transient spreading module. After transient fusion and recovery, a physical-based network is employed to reconstruct the hidden volume. To tackle the substantial noise inherent in transient videos, we propose a wave-based loss function to impose constraints within the phasor field. Besides, we introduce a new dataset, comprising synthetic videos for training and real-world videos for evaluation. Extensive experiments showcase the superior performance of our method on both synthetic data and real-world data captured by different imaging setups. The code and data are available at https://github.com/Depth2World/Dynamic_NLOS.
</div>

**Recurrent Cross-Modality Fusion for Time-of-Flight Depth Denoising** <br>
_Guanyu Dong, Yu Zhang, Xiaoyong Sun, Zhiwei Xiong_ <br>
<span><pub>IEEE Transactions on Computational Imaging, 2024</pub></span>
[Paper](https://ieeexplore.ieee.org/document/10750330){:target="\_blank"} |
[Code](https://github.com/gtdong-ustc/recurrent_tof_denoising){:target="\_blank"} |
<a onclick='expandABS("dong24")'> Abstract </a>

<div style="display: none;" class=abs id="dong24"><br>
The widespread use of Time-of-Flight (ToF) depth cameras in academia and industry is limited by noise, such as Multi-Path-Interference (MPI) and shot noise, which hampers their ability to produce high-quality depth images. Learning-based ToF denoising methods currently in existence often face challenges in delivering satisfactory performance in complex scenes. This is primarily attributed to the impact of multiple reflected signals on the formation of MPI, rendering it challenging to predict MPI directly through spatially-varying convolutions. To address this limitation, we adopt a recurrent architecture that exploits the prior that MPI is decomposable into an additive combination of the geometric information for the neighboring pixels. Our approach employs a Gated Recurrent Unit (GRU) based network to estimate a long-distance aggregation process, simplifying the MPI removal and updating depth correction over multiple steps. Additionally, we introduce a global restoration module and a local update module to fuse depth and amplitude features, which improves denoising performance and prevents structural distortions. Experimental results on both synthetic and real-world datasets demonstrate the superiority of our approach over state-of-the-art methods.
</div>


**Exploiting Dual-Correlation for Multi-frame Time-of-Flight Denoising** <br>
_Guanyu Dong, Yu Zhang, Xiaoyong Sun, Zhiwei Xiong_ <br>
<span><pub>European Conference on Computer Vision (ECCV), 2024</pub></span> <br>
[Paper](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03302.pdf){:target="\_blank"} |
[Code](){:target="\_blank"} |
<a onclick='expandABS("li24")'> Abstract </a>

<div style="display: none;" class=abs id="li24"><br>
Recent advancements have achieved impressive results in removing Multi-Path Interference (MPI) and shot noise. However, these methods only utilize a single frame of ToF data, neglecting the correlation between frames. The multi-frame ToF denoising is still underexplored. In this paper, we propose the first learning-based framework for multi-frame ToF denoising. Different from previous frameworks, ours leverages the correlation between inter frames to guide the ToF noise removal with a confidence map. Specifically, we introduce a Dual-Correlation Estimation Module, which exploits both intra- and inter-correlation. The intra-correlation explicitly establishes the relevance between the spatial positions of geometric objects within the scene, aiding in depth residual initialization. The inter-correlation discerns variations in ToF noise distribution across different frames, thereby locating the areas with strong noise. To further leverage dual-correlation, we introduce a Confidence-guided Residual Regression Module to predict a confidence map, which guides the residual regression to prioritize the regions with strong ToF noise. The experimental evaluations have consistently shown that our approach outperforms other ToF denoising methods, highlighting its superior performance in effectively reducing strong ToF noise.


**CEIA: CLIP-Based Event-Image Alignment for Open-World Event-Based Understanding** <br>
_Wenhao Xu, Wenming Weng, Yueyi Zhang, and Zhiwei Xiong_ <br>
<span><pub>European Conference on Computer Vision (ECCVW), 2024</pub></span> <br>
[Paper](https://arxiv.org/pdf/2407.06611.pdf){:target="\_blank"} |
[Code](){:target="\_blank"} |
<a onclick='expandABS("xu24")'> Abstract </a>

<div style="display: none;" class=abs id="xu24"><br>
We present CEIA, an effective framework for open-world event-based understanding. Currently training a large event-text model still poses a huge challenge due to the shortage of paired event-text data. In response to this challenge, CEIA learns to align event and image data as an alternative instead of directly aligning event and text data. Specifically, we leverage the rich event-image datasets to learn an event embedding space aligned with the image space of CLIP through contrastive learning. In this way, event and text data are naturally aligned via using image data as a bridge. Particularly, CEIA offers two distinct advantages. First, it allows us to take full advantage of the existing event-image datasets to make up the shortage of large-scale event-text datasets. Second, leveraging more training data, it also exhibits the flexibility to boost performance, ensuring scalable capability. In highlighting the versatility of our framework, we make extensive evaluations through a diverse range of event-based multi-modal applications, such as object recognition, event-image retrieval, event-text retrieval, and domain adaptation. The outcomes demonstrate CEIA's distinct zero-shot superiority over existing methods on these applications.


**Event-Adapted Video Super-Resolution** <br>
_Zeyu Xiao, Dachun Kai, Yueyi Zhang, Zheng-Jun Zha, Xiaoyan Sun, Zhiwei Xiong_ <br>
<span><pub>European Conference on Computer Vision (ECCV), 2024</pub></span> <br>
[Paper](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05857.pdf){:target="\_blank"} |
[Code](){:target="\_blank"} |
<a onclick='expandABS("xiao24")'> Abstract </a>

<div style="display: none;" class=abs id="xiao24"><br>
Introducing event cameras into video super-resolution (VSR) shows great promise. In practice, however, integrating event data as a new modality necessitates a laborious model architecture design. This not only consumes substantial time and effort but also disregards valuable insights from successful existing VSR models. Furthermore, the resource-intensive process of retraining these newly designed structures exacerbates the challenge. In this paper, inspired by recent success of parameter-efficient tuning in reducing the number of trainable parameters of a pre-trained model for downstream tasks, we introduce the Event AdapTER (EATER) for VSR. EATER efficiently utilizes pre-trained VSR model knowledge at the feature level through two lightweight and trainable components: the event-adapted alignment (EAA) unit and the event-adapted fusion (EAF) unit. The EAA unit aligns multiple frames based on the event stream in a coarse-to-fine manner, while the EAF unit efficiently fuses frames with the event stream through a multi-scaled design. Thanks to both units, EATER outperforms the full fine-tuning paradigm. Comprehensive experiments demonstrate the effectiveness of EATER, achieving superior results with parameter efficiency.


**Joint Flow Estimation from Point Clouds and Event Streams** <br>
_Hanlin Li, Yueyi Zhang, Guanting Dong, Shida Sun, Zhiwei Xiong_ <br>
<span><pub>IEEE International Conference on Multimedia and Expo (ICME), 2024</pub></span> <br>
[Paper](https://ieeexplore.ieee.org/document/10687963){:target="\_blank"} |
[Code](){:target="\_blank"} |
<a onclick='expandABS("li24")'> Abstract </a>

<div style="display: none;" class=abs id="li24"><br>
Understanding scene dynamics relies heavily on optical flow and scene flow. Most existing flow estimation methods use low-rate RGB images and point clouds, and match the frames geometrically. However, this approach faces challenges in real-world scenes with intricate motion, occlusion, and noise. To tackle this problem, we combine point clouds with events, which introduce dynamic inter-frame information. We propose a bi-stream neural network that jointly estimates optical flow and scene flow. The event branch extracts dynamic information and estimates optical flow, while the point branch captures scene structure and estimate scene flow. A Spatio-temporal Fusion Block is introduced to fuse the complementary information from points and events. Additionally, we adopt a result-level fusion strategy for direct refinement between the flow predictions of the two branches. We evaluate our model on the real-world datasets DSEC and MVSEC. The experimental results demonstrate superior performance compared to existing methods.


**Depth From Asymmetric Frame-Event Stereo: A Divide-and-Conquer Approach** <br>
_Xihao Chen, Wenming Weng, Yueyi Zhang, Zhiwei Xiong_ <br>
<span><pub>IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2024</pub></span> <br>
[Paper](https://openaccess.thecvf.com/content/WACV2024/html/Chen_Depth_From_Asymmetric_Frame-Event_Stereo_A_Divide-and-Conquer_Approach_WACV_2024_paper.html){:target="\_blank"} |
[Code](https://github.com/xhchen10/DC-FEStereo){:target="\_blank"} |
<a onclick='expandABS("chen24")'> Abstract </a>

<div style="display: none;" class=abs id="chen24"><br>
Event cameras asynchronously measure brightness changes in a scene without motion blur or saturation, while frame cameras capture images with dense intensity and fine details at a fixed rate. The exclusive advantages of the two modalities make depth estimation from Stereo Asymmetric Frame-Event (SAFE) systems appealing. However, due to the inevitable information absence of one modality in certain challenging regions, existing stereo matching methods lose efficacy for asymmetric inputs from SAFE systems. In this paper, we propose a divide-and-conquer approach that decomposes depth estimation from SAFE systems into three sub-tasks, i.e., frame-event stereo matching, frame-based Structure-from-Motion (SfM), and event-based SfM. In this way, the above challenging regions are addressed by monocular SfM, which estimates robust depth with two views belonging to the same functioning modality. Moreover, we propose a dual sampling strategy to construct cost volumes with identical spatial locations and depth hypotheses for different sub-tasks, which enables sub-task fusion at the cost volume level. To tackle the occlusion issue raised by the sampling strategy, we further introduce a temporal fusion scheme to utilize long-term sequential inputs with multi-view information. Experimental results validate the superior performance of our method over existing solutions.
</div>

<!-- **Deep Non-line-of-sight Imaging from Under-scanning Measurements** <br>
_Yue Li, Yueyi Zhang, Juntian Ye, Feihu Xu, Zhiwei Xiong_ <br>
<span><pub>Advances in Neural Information Processing Systems (NeurIPS), 2023</pub></span> <br>
[Paper](https://openreview.net/forum?id=JCN9YsZiwB){:target="\_blank"} |
[Code](https://github.com/Depth2World/Under-scanning_NLOS){:target="\_blank"} |
<a onclick='expandABS("li23nips")'> Abstract </a>

<div style="display: none;" class=abs id="li23nips"><br>
Active confocal non-line-of-sight (NLOS) imaging has successfully enabled seeing around corners relying on high-quality transient measurements. However, acquiring spatial-dense transient measurement is time-consuming, raising the question of how to reconstruct satisfactory results from under-scanning measurements (USM). The existing solutions, involving the traditional algorithms, however, are hindered by unsatisfactory results or long computing times. To this end, we propose the first deep-learning-based approach to NLOS imaging from USM. Our proposed end-to-end network is composed of two main components: the transient recovery network (TRN) and the volume reconstruction network (VRN). Specifically, TRN takes the under-scanning measurements as input, utilizes a multiple kernel feature extraction module and a multiple feature fusion module, and outputs sufficient-scanning measurements at the high-spatial resolution. Afterwards, VRN incorporates the linear physics prior of the light-path transport model and reconstructs the hidden volume representation. Besides, we introduce regularized constraints that enhance the perception of more local details while suppressing smoothing effects. The proposed method achieves superior performance on both synthetic data and public real-world data, as demonstrated by extensive experimental results with different under-scanning grids. Moreover, the proposed method delivers impressive robustness at an extremely low scanning grid (i.e., 8x8) and offers high-speed inference (i.e., 50 times faster than the existing iterative solution).

</div> -->
<!-- 
---

**[▲ 2024 New](/3D-event/2024-New){: style="color: rgb(191, 0, 0)"}**
**[▶ Active 3D](/3D-event/active3d){: style="color: rgb(191, 0, 0)"}**
**[▶ Passive 3D](/3D-event/passive3d){: style="color: rgb(191, 0, 0)"}**
**[▶ Event](/3D-event/event){: style="color: rgb(191, 0, 0)"}**
**[▶ Point Cloud](/3D-event/point-cloud){: style="color: rgb(191, 0, 0)"}** -->
