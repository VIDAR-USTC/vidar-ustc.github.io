---
title: "Research Topic: Biomedical Image Analysis"
excerpt: "Biomedical Image Analysis"
author_profile: True
permalink: /biomedical-imaging/2024-New.html
---

**[▼ 2024 New](/biomedical-imaging/2024-New){: style="color: rgb(191, 0, 0)"}**
**[▶ Segmentation](/biomedical-imaging/segmentation){: style="color: rgb(191, 0, 0)"}**
**[▶ Registration](/biomedical-imaging/registration){: style="color: rgb(191, 0, 0)"}**
**[▶ Restoration](/biomedical-imaging/restoration){: style="color: rgb(191, 0, 0)"}**
**[▶ Misc.](/biomedical-imaging/misc){: style="color: rgb(191, 0, 0)"}**

## 2024 New

**Unsupervised Domain Adaptation for EM Image Denoising with Invertible Networks** <br>
_Shiyu Deng, Yinda Chen, Wei Huang, Ruobing Zhang, Zhiwei Xiong_ <br>
<span><pub> IEEE Transactions on Medical Imaging (T-MI), 2024</pub></span> <br>
[Paper](https://ieeexplore.ieee.org/document/10604846){:target="\_blank"} |
[Code](https://github.com/sydeng99/DADn){:target="\_blank"} |
<span style="color: green;"><span>
<a onclick='expandABS("shiyutmi24")'> Abstract </a>

<div style="display: none;" class=abs id="shiyutmi24"><br>
Electron microscopy (EM) image denoising is critical for visualization and subsequent analysis. Despite the remarkable achievements of deep learning-based non-blind denoising methods, their performance drops significantly when domain shifts exist between the training and testing data. To address this issue, unpaired blind denoising methods have been proposed. However, these methods heavily rely on image-to-image translation and neglect the inherent characteristics of EM images, limiting their overall denoising performance. In this paper, we propose the first unsupervised domain adaptive EM image denoising method, which is grounded in the observation that EM images from similar samples share common content characteristics. Specifically, we first disentangle the content representations and the noise components from noisy images and establish a shared domain-agnostic content space via domain alignment to bridge the synthetic images (source domain) and the real images (target domain). To ensure precise domain alignment, we further incorporate domain regularization by enforcing that: the pseudo-noisy images, reconstructed using both content representations and noise components, accurately capture the characteristics of the noisy images from which the noise components originate, all while maintaining semantic consistency with the noisy images from which the content representations originate. To guarantee lossless representation decomposition and image reconstruction, we introduce disentanglement-reconstruction invertible networks. Finally, the reconstructed pseudo-noisy images, paired with their corresponding clean counterparts, serve as valuable training data for the denoising network. Extensive experiments on synthetic and real EM datasets demonstrate the superiority of our method in terms of image restoration quality and downstream neuron segmentation accuracy. Our code is publicly available at https://github.com/sydeng99/DADn.
</div>

**PQ-SAM: Post-training Quantization for Segment Anything Model** <br>
_Xiaoyu Liu, Xin Ding, Lei Yu, Yuanyuan Xi, Wei Li, Zhijun Tu, Jie Hu,Hanting Chen, Baoqun Yin, Zhiwei Xiong_ <br>
<span><pub>European Conference on Computer Vision (ECCV), 2024</pub></span> <br>
[Paper](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01627.pdf){:target="\_blank"} |
<span style="color: green;"><span>
<a onclick='expandABS("shiyutmi24")'> Abstract </a>

<div style="display: none;" class=abs id="shiyutmi24"><br>
Segment anything model (SAM) is a promising prompt-guided vision foundation model to segment objects of interest. However, the extensive computational requirements of SAM have limited its applicability in resource-constraint edge devices. Post-training quantization (PTQ) is an effective potential for fast-deploying SAM. Nevertheless, SAM’s billion-scale pretraining creates a highly asymmetric activation distribution with detrimental outliers in excessive channels, resulting in significant performance degradation of the low-bit PTQ. In this paper, we propose PQ-SAM, the first PTQ method customized for SAM. To achieve a quantization-friendly tensor-wise distribution, PQ-SAM incorporates a novel grouped activation distribution transformation (GADT) based on a two-stage outlier hierarchical clustering (OHC) scheme to scale and shift each channel. Firstly, OHC identifies and truncates extreme outliers to reduce the scale variance of different channels. Secondly, OHC iteratively allocates learnable shifting and scaling sizes to each group of channels with similar distributions, reducing the number of learnable parameters and easing the optimization difficulty. These shifting and scaling sizes are used to adjust activation channels, and jointly optimized with quantization step sizes for optimal results. Extensive experiments demonstrate that PQ-SAM outperforms existing PTQ methods on nine zero-shot datasets, and pushes the 4-bit PTQ of SAM to a usable level.
</div>


**Joint EM Image Denoising and Segmentation with Instance-aware Interaction** <br>
_Zhicheng Wang, Jiacheng Li, Yinda Chen, Jiateng Shou, Shiyu Deng, Wei Huang, Zhiwei Xiong_ <br>
<span><pub> International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI), 2024</pub></span> <br>
[Paper](https://papers.miccai.org/miccai-2024/paper/1351_paper.pdf){:target="\_blank"} |
[Code](https://github.com/zhichengwang-tri/EM-DenoiSeg){:target="\_blank"} |
<span style="color: green;"><span>
<a onclick='expandABS("Zhicheng24")'> Abstract </a>

<div style="display: none;" class=abs id="Zhicheng24"><br>
In large scale electron microscopy(EM), the demand for rapid imaging often results in significant amounts of imaging noise, which considerably compromise segmentation accuracy. While conventional approaches typically incorporate denoising as a preliminary stage, there is limited exploration into the potential synergies between denoising and segmentation processes. To bridge this gap, we propose an instance-aware interaction framework to tackle EM image denoising and segmentation simultaneously, aiming at mutual enhancement between the two tasks. Specifically, our framework comprises three components: a denoising network, a segmentation network, and a fusion network facilitating feature-level interaction. Firstly, the denoising network mitigates noise degradation. Subsequently, the segmentation network learns an instance-level affinity prior, encoding vital spatial structural information. Finally, in the fusion network, we propose a novel Instance-aware Embedding Module (IEM) to utilize vital spatial structure information from segmentation features for denoising. IEM enables interaction between the two tasks within a unified framework, which also facilitates implicit feedback from denoising for segmentation with a joint training mechanism. Through extensive experiments across multiple datasets, our framework demonstrates substantial performance improvements over existing solutions. Moreover, our framework exhibits strong generalization capabilities across different network architectures. Code is available at https://github.com/zhichengwang-tri/EM-DenoiSeg.
</div>



**BIMCV-R: A Landmark Dataset for 3D CT Text-Image Retrieval** <br>
_Yinda Chen, Che Liu, Xiaoyu Liu, Rossella Arcucci, Zhiwei Xiong_ <br>
<span><pub> International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI), 2024</pub></span> <br>
[Paper](https://papers.miccai.org/miccai-2024/paper/1194_paper.pdf){:target="\_blank"} |
[Dataset](https://huggingface.co/datasets/cyd0806/BIMCV-R){:target="\_blank"} |
<a onclick='expandABS("Yinda_micai24")'> Abstract </a>

<div style="display: none;" class=abs id="Yinda_micai24"><br>
The burgeoning integration of 3D medical imaging into healthcare has led to a substantial increase in the workload of medical professionals. To assist clinicians in their diagnostic processes and alleviate their workload, the development of a robust system for retrieving similar case studies presents a viable solution. While the concept holds great promise, the field of 3D medical text-image retrieval is currently limited by the absence of robust evaluation benchmarks and curated datasets. To remedy this, our study presents a groundbreaking dataset, BIMCV-R (This dataset will be released upon acceptance.), which includes an extensive collection of 8,069 3D CT volumes, encompassing over 2 million slices, paired with their respective radiological reports. Expanding upon the foundational work of our dataset, we craft a retrieval strategy, MedFinder. This approach employs a dual-stream network architecture, harnessing the potential of large language models to advance the field of medical image retrieval beyond existing text-image retrieval solutions. It marks our preliminary step towards developing a system capable of facilitating text-to-image, image-to-text, and keyword-based retrieval tasks.
</div>




**Learning Large-Factor EM Image Super-Resolution with Generative Priors** <br>
_Jiateng Shou, Zeyu Xiao, Shiyu Deng, Wei Huang, Peiyao Shi, Ruobing Zhang, Zhiwei Xiong, Feng Wu_ <br>
<span><pub> IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024</pub></span> <br>
[Paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Shou_Learning_Large-Factor_EM_Image_Super-Resolution_with_Generative_Priors_CVPR_2024_paper.pdf){:target="\_blank"} |
[Code](https://github.com/jtshou/GPEMSR){:target="\_blank"} |
<a onclick='expandABS("Jiateng_cvpr24")'> Abstract </a>

<div style="display: none;" class=abs id="Jiateng_cvpr24"><br>
As the mainstream technique for capturing images of biological specimens at nanometer resolution electron microscopy (EM) is extremely time-consuming for scanning wide field-of-view (FOV) specimens. In this paper we investigate a challenging task of large-factor EM image super-resolution (EMSR) which holds great promise for reducing scanning time relaxing acquisition conditions and expanding imaging FOV. By exploiting the repetitive structures and volumetric coherence of EM images we propose the first generative learning-based framework for large-factor EMSR. Specifically motivated by the predictability of repetitive structures and textures in EM images we first learn a discrete codebook in the latent space to represent high-resolution (HR) cell-specific priors and a latent vector indexer to map low-resolution (LR) EM images to their corresponding latent vectors in a generative manner. By incorporating the generative cell-specific priors from HR EM images through a multi-scale prior fusion module we then deploy multi-image feature alignment and fusion to further exploit the inter-section coherence in the volumetric EM data. Extensive experiments demonstrate that our proposed framework outperforms advanced single-image and video super-resolution methods for 8x and 16x EMSR (ie with 64 times and 256 times less data acquired respectively) achieving superior visual reconstruction quality and downstream segmentation accuracy on benchmark EM datasets. Code is available at https://github. com/jtshou/GPEMSR.
</div>

**Cross-Dimension Affinity Distillation for 3D EM Neuron Segmentation** <br>
_Xiaoyu Liu, Miaomiao Cai, Yinda Chen, Yueyi Zhang, Te Shi, Ruobing Zhang, Xuejin Chen, Zhiwei Xiong_ <br>
<span><pub> IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024</pub></span> <br>
[Paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Cross-Dimension_Affinity_Distillation_for_3D_EM_Neuron_Segmentation_CVPR_2024_paper.pdf){:target="\_blank"} |
[Code](https://github.com/liuxy1103/CAD){:target="\_blank"} |
<a onclick='expandABS("Xiaoyu_cvpr24")'> Abstract </a>

<div style="display: none;" class=abs id="Xiaoyu_cvpr24"><br>
Accurate 3D neuron segmentation from electron microscopy (EM) volumes is crucial for neuroscience research. However the complex neuron morphology often leads to over-merge and over-segmentation results. Recent advancements utilize 3D CNNs to predict a 3D affinity map with improved accuracy but suffer from two challenges: high computational cost and limited input size especially for practical deployment for large-scale EM volumes. To address these challenges we propose a novel method to leverage lightweight 2D CNNs for efficient neuron segmentation. Our method employs a 2D Y-shape network to generate two embedding maps from adjacent 2D sections which are then converted into an affinity map by measuring their embedding distance. While the 2D network better captures pixel dependencies inside sections with larger input sizes it overlooks inter-section dependencies. To overcome this we introduce a cross-dimension affinity distillation (CAD) strategy that transfers inter-section dependency knowledge from a 3D teacher network to the 2D student network by ensuring consistency between their output affinity maps. Additionally we design a feature grafting interaction (FGI) module to enhance knowledge transfer by grafting embedding maps from the 2D student onto those from the 3D teacher. Extensive experiments on multiple EM neuron segmentation datasets including a newly built one by ourselves demonstrate that our method achieves superior performance over state-of-the-art methods with only 1/20 inference latency.
</div>

**Domain Adaptive Synapse Detection with Weak Point Annotations** <br>
_Qi Chen, Wei Huang, Yueyi Zhang, Zhiwei Xiong_ <br>
<span><pub> IEEE International Symposium on Biomedical Imaging (ISBI), 2024</pub></span> <br>
<span><highlighted>Winner</highlighted> of ISBI 2023 WASPSYN Challenge on 3D Synapse Detection<span> <br>
[Paper](https://arxiv.org/abs/2308.16461){:target="\_blank"} |
[Code](https://github.com/qic999/AdaSyn){:target="\_blank"} |
<a onclick='expandABS("Qi_ISBI24")'> Abstract </a>

<div style="display: none;" class=abs id="Qi_ISBI24"><br>
The development of learning-based methods has greatly improved the detection of synapses from electron microscopy (EM) images. However, training a model for each dataset is time-consuming and requires extensive annotations. Additionally, it is difficult to apply a learned model to data from different brain regions due to variations in data distributions. In this paper, we present AdaSyn, a two-stage segmentation-based framework for domain adaptive synapse detection with weak point annotations. In the first stage, we address the detection problem by utilizing a segmentation-based pipeline to obtain synaptic instance masks. In the second stage, we improve model generalizability on target data by regenerating square masks to get high-quality pseudo labels. Benefiting from our high-accuracy detection results, we introduce the distance nearest principle to match paired pre-synapses and post-synapses. In the WASPSYN challenge at ISBI 2023, our method ranks the 1st place.
</div>

**Learning Multiscale Consistency for Self-supervised Electron Microscopy Instance Segmentation** <br>
_Yinda Chen, Wei Huang, Xiaoyu Liu, Shiyu Deng, Qi Chen, Zhiwei Xiong_ <br>
<span><pub>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2024</pub></span> <br>
[Paper](https://ieeexplore.ieee.org/document/10446055){:target="\_blank"} |
[Code](https://github.com/ydchen0806/MS-Con-EM-Seg){:target="\_blank"} |
<a onclick='expandABS("Yinda_icassp")'> Abstract </a>

<div style="display: none;" class=abs id="Yinda_icassp"><br>
Instance segmentation in electron microscopy (EM) volumes poses a significant challenge due to the complex morphology of instances and insufficient annotations. Self-supervised learning has recently emerged as a promising solution, enabling the acquisition of prior knowledge of cellular tissue structures that are essential for EM instance segmentation. However, existing pretraining methods often lack the ability to capture complex visual patterns and relationships between voxels, which results in the acquired prior knowledge being insufficient for downstream EM analysis tasks. In this paper, we propose a novel pretraining framework that leverages multiscale visual representations to capture both voxel-level and feature-level consistency in EM volumes. Specifically, our framework enforces voxel-level consistency between the outputs of a Siamese network by a reconstruction function, and incorporates a cross-attention mechanism for soft feature matching to achieve fine-grained feature-level consistency. Moreover, we propose a contrastive learning scheme on the feature pyramid to extract discriminative features across multiple scales. We extensively pretrain our method on four large-scale EM datasets, achieving promising performance improvements in representative tasks of neuron and mitochondria instance segmentation.
</div>

**Test-Time Adaptation via Style and Structure Guidance for Histological Image Registration** <br>
_Shenglong Zhou, Zhiwei Xiong, Feng Wu_ <br>
<span><pub> AAAI Conference on Artificial Intelligence (AAAI), 2024</pub></span> <br>
[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/28601){:target="\_blank"} |
<a onclick='expandABS("Shenglong_aaai24")'> Abstract </a>

<div style="display: none;" class=abs id="Shenglong_aaai24"><br>
Image registration plays a crucial role in histological image analysis, encompassing tasks like multi-modality fusion and disease grading. Traditional registration methods optimize objective functions for each image pair, yielding reliable accuracy but demanding heavy inference burdens. Recently, learning-based registration methods utilize networks to learn the optimization process during training and apply a one-step forward process during testing. While these methods offer promising registration performance with reduced inference time, they remain sensitive to appearance variances and local structure changes commonly encountered in histological image registration scenarios. In this paper, for the first time, we propose a novel test-time adaptation method for histological image registration, aiming to improve the generalization ability of learning-based methods. Specifically, we design two operations, style guidance and shape guidance, for the test-time adaptation process. The former leverages style representations encoded by feature statistics to address the issue of appearance variances, while the latter incorporates shape representations encoded by HOG features to improve registration accuracy in regions with structural changes. Furthermore, we consider the continuity of the model during the test-time adaptation process. Different from the previous methods initialized by a given trained model, we introduce a smoothing strategy to leverage historical models for better generalization. We conduct experiments with several representative learning-based backbones on the public histological dataset, demonstrating the superior registration performance of our test-time adaptation method.
</div>

---

**[▲ 2024 New](/biomedical-imaging/2024-New){: style="color: rgb(191, 0, 0)"}**
**[▶ Segmentation](/biomedical-imaging/segmentation){: style="color: rgb(191, 0, 0)"}**
**[▶ Registration](/biomedical-imaging/registration){: style="color: rgb(191, 0, 0)"}**
**[▶ Restoration](/biomedical-imaging/restoration){: style="color: rgb(191, 0, 0)"}**
**[▶ Misc.](/biomedical-imaging/misc){: style="color: rgb(191, 0, 0)"}**
