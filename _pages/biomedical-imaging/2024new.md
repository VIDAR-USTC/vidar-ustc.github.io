---
title: "Research Topic: Biomedical Image Analysis"
excerpt: "Biomedical Image Analysis"
author_profile: True
permalink: /biomedical-imaging/2024-New.html
---

**[▼ 2024 New](/biomedical-imaging/2024-New){: style="color: rgb(191, 0, 0)"}**
**[▶ Segmentation](/biomedical-imaging/segmentation){: style="color: rgb(191, 0, 0)"}**
**[▶ Registration](/biomedical-imaging/registration){: style="color: rgb(191, 0, 0)"}**
**[▶ Restoration](/biomedical-imaging/restoration){: style="color: rgb(191, 0, 0)"}**
**[▶ Misc.](/biomedical-imaging/misc){: style="color: rgb(191, 0, 0)"}**

## 2024 New

**Unsupervised Domain Adaptation for EM Image Denoising with Invertible Networks** <br>
_Shiyu Deng, Yinda Chen, Wei Huang, Ruobing Zhang, Zhiwei Xiong_ <br>
<span><pub> IEEE Transactions on Medical Imaging (T-MI), 2024</pub></span> <br>
<span style="color: green;"><span style="color: green;">Coming soon! </span> </span>

**Joint EM Image Denoising and Segmentation with Instance-aware Interaction** <br>
_Zhicheng Wang, Jiacheng Li, Yinda Chen, Jiateng Shou, Shiyu Deng, Wei Huang, Zhiwei Xiong_ <br>
<span><pub> International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI), 2024</pub></span> <br>
<span style="color: green;"><span style="color: green;">Coming soon! </span> </span>

**BIMCV-R: A Landmark Dataset for 3D CT Text-Image Retrieval** <br>
_Yinda Chen, Che Liu, Xiaoyu Liu, Rossella Arcucci, Zhiwei Xiong_ <br>
<span><pub> International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI), 2024</pub></span> <br>
[Paper](https://arxiv.org/abs/2403.15992){:target="\_blank"} |
<a onclick='expandABS("Yinda_micai24")'> Abstract </a>

<div style="display: none;" class=abs id="Yinda_micai24"><br>
The burgeoning integration of 3D medical imaging into healthcare has led to a substantial increase in the workload of medical professionals. To assist clinicians in their diagnostic processes and alleviate their workload, the development of a robust system for retrieving similar case studies presents a viable solution. While the concept holds great promise, the field of 3D medical text-image retrieval is currently limited by the absence of robust evaluation benchmarks and curated datasets. To remedy this, our study presents a groundbreaking dataset, BIMCV-R (This dataset will be released upon acceptance.), which includes an extensive collection of 8,069 3D CT volumes, encompassing over 2 million slices, paired with their respective radiological reports. Expanding upon the foundational work of our dataset, we craft a retrieval strategy, MedFinder. This approach employs a dual-stream network architecture, harnessing the potential of large language models to advance the field of medical image retrieval beyond existing text-image retrieval solutions. It marks our preliminary step towards developing a system capable of facilitating text-to-image, image-to-text, and keyword-based retrieval tasks.
</div>

**Learning Large-Factor EM Image Super-Resolution with Generative Priors** <br>
_Jiateng Shou, Zeyu Xiao, Shiyu Deng, Wei Huang, Peiyao Shi, Ruobing Zhang, Zhiwei Xiong, Feng Wu_ <br>
<span><pub> IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024</pub></span> <br>
[Paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Shou_Learning_Large-Factor_EM_Image_Super-Resolution_with_Generative_Priors_CVPR_2024_paper.pdf){:target="\_blank"} |
[Code](https://github.com/jtshou/GPEMSR){:target="\_blank"} |
<a onclick='expandABS("Jiateng_cvpr24")'> Abstract </a>

<div style="display: none;" class=abs id="Jiateng_cvpr24"><br>
As the mainstream technique for capturing images of biological specimens at nanometer resolution electron microscopy (EM) is extremely time-consuming for scanning wide field-of-view (FOV) specimens. In this paper we investigate a challenging task of large-factor EM image super-resolution (EMSR) which holds great promise for reducing scanning time relaxing acquisition conditions and expanding imaging FOV. By exploiting the repetitive structures and volumetric coherence of EM images we propose the first generative learning-based framework for large-factor EMSR. Specifically motivated by the predictability of repetitive structures and textures in EM images we first learn a discrete codebook in the latent space to represent high-resolution (HR) cell-specific priors and a latent vector indexer to map low-resolution (LR) EM images to their corresponding latent vectors in a generative manner. By incorporating the generative cell-specific priors from HR EM images through a multi-scale prior fusion module we then deploy multi-image feature alignment and fusion to further exploit the inter-section coherence in the volumetric EM data. Extensive experiments demonstrate that our proposed framework outperforms advanced single-image and video super-resolution methods for 8x and 16x EMSR (ie with 64 times and 256 times less data acquired respectively) achieving superior visual reconstruction quality and downstream segmentation accuracy on benchmark EM datasets. Code is available at https://github. com/jtshou/GPEMSR.
</div>

**Cross-Dimension Affinity Distillation for 3D EM Neuron Segmentation** <br>
_Xiaoyu Liu, Miaomiao Cai, Yinda Chen, Yueyi Zhang, Te Shi, Ruobing Zhang, Xuejin Chen, Zhiwei Xiong_ <br>
<span><pub> IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024</pub></span> <br>
[Paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Cross-Dimension_Affinity_Distillation_for_3D_EM_Neuron_Segmentation_CVPR_2024_paper.pdf){:target="\_blank"} |
[Code](https://github.com/liuxy1103/CAD){:target="\_blank"} |
<a onclick='expandABS("Xiaoyu_cvpr24")'> Abstract </a>

<div style="display: none;" class=abs id="Xiaoyu_cvpr24"><br>
Accurate 3D neuron segmentation from electron microscopy (EM) volumes is crucial for neuroscience research. However the complex neuron morphology often leads to over-merge and over-segmentation results. Recent advancements utilize 3D CNNs to predict a 3D affinity map with improved accuracy but suffer from two challenges: high computational cost and limited input size especially for practical deployment for large-scale EM volumes. To address these challenges we propose a novel method to leverage lightweight 2D CNNs for efficient neuron segmentation. Our method employs a 2D Y-shape network to generate two embedding maps from adjacent 2D sections which are then converted into an affinity map by measuring their embedding distance. While the 2D network better captures pixel dependencies inside sections with larger input sizes it overlooks inter-section dependencies. To overcome this we introduce a cross-dimension affinity distillation (CAD) strategy that transfers inter-section dependency knowledge from a 3D teacher network to the 2D student network by ensuring consistency between their output affinity maps. Additionally we design a feature grafting interaction (FGI) module to enhance knowledge transfer by grafting embedding maps from the 2D student onto those from the 3D teacher. Extensive experiments on multiple EM neuron segmentation datasets including a newly built one by ourselves demonstrate that our method achieves superior performance over state-of-the-art methods with only 1/20 inference latency.
</div>

**Domain Adaptive Synapse Detection with Weak Point Annotations** <br>
_Qi Chen, Wei Huang, Yueyi Zhang, Zhiwei Xiong_ <br>
<span><pub> IEEE International Symposium on Biomedical Imaging (ISBI), 2024</pub></span> <br>
<span><highlighted>Winner</highlighted> of ISBI 2023 WASPSYN Challenge on 3D Synapse Detection<span> <br>
[Paper](https://arxiv.org/abs/2308.16461){:target="\_blank"} |
[Code](https://github.com/qic999/AdaSyn){:target="\_blank"} |
<a onclick='expandABS("Qi_ISBI24")'> Abstract </a>

<div style="display: none;" class=abs id="Qi_ISBI24"><br>
The development of learning-based methods has greatly improved the detection of synapses from electron microscopy (EM) images. However, training a model for each dataset is time-consuming and requires extensive annotations. Additionally, it is difficult to apply a learned model to data from different brain regions due to variations in data distributions. In this paper, we present AdaSyn, a two-stage segmentation-based framework for domain adaptive synapse detection with weak point annotations. In the first stage, we address the detection problem by utilizing a segmentation-based pipeline to obtain synaptic instance masks. In the second stage, we improve model generalizability on target data by regenerating square masks to get high-quality pseudo labels. Benefiting from our high-accuracy detection results, we introduce the distance nearest principle to match paired pre-synapses and post-synapses. In the WASPSYN challenge at ISBI 2023, our method ranks the 1st place.
</div>

**Learning Multiscale Consistency for Self-supervised Electron Microscopy Instance Segmentation** <br>
_Yinda Chen, Wei Huang, Xiaoyu Liu, Shiyu Deng, Qi Chen, Zhiwei Xiong_ <br>
<span><pub>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2024</pub></span> <br>
[Paper](https://ieeexplore.ieee.org/document/10446055){:target="\_blank"} |
[Code](https://github.com/ydchen0806/MS-Con-EM-Seg){:target="\_blank"} |
<a onclick='expandABS("Yinda_icassp")'> Abstract </a>

<div style="display: none;" class=abs id="Yinda_icassp"><br>
Instance segmentation in electron microscopy (EM) volumes poses a significant challenge due to the complex morphology of instances and insufficient annotations. Self-supervised learning has recently emerged as a promising solution, enabling the acquisition of prior knowledge of cellular tissue structures that are essential for EM instance segmentation. However, existing pretraining methods often lack the ability to capture complex visual patterns and relationships between voxels, which results in the acquired prior knowledge being insufficient for downstream EM analysis tasks. In this paper, we propose a novel pretraining framework that leverages multiscale visual representations to capture both voxel-level and feature-level consistency in EM volumes. Specifically, our framework enforces voxel-level consistency between the outputs of a Siamese network by a reconstruction function, and incorporates a cross-attention mechanism for soft feature matching to achieve fine-grained feature-level consistency. Moreover, we propose a contrastive learning scheme on the feature pyramid to extract discriminative features across multiple scales. We extensively pretrain our method on four large-scale EM datasets, achieving promising performance improvements in representative tasks of neuron and mitochondria instance segmentation.
</div>

**Test-Time Adaptation via Style and Structure Guidance for Histological Image Registration** <br>
_Shenglong Zhou, Zhiwei Xiong, Feng Wu_ <br>
<span><pub> AAAI Conference on Artificial Intelligence (AAAI), 2024</pub></span> <br>
[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/28601){:target="\_blank"} |
<a onclick='expandABS("Shenglong_aaai24")'> Abstract </a>

<div style="display: none;" class=abs id="Shenglong_aaai24"><br>
Image registration plays a crucial role in histological image analysis, encompassing tasks like multi-modality fusion and disease grading. Traditional registration methods optimize objective functions for each image pair, yielding reliable accuracy but demanding heavy inference burdens. Recently, learning-based registration methods utilize networks to learn the optimization process during training and apply a one-step forward process during testing. While these methods offer promising registration performance with reduced inference time, they remain sensitive to appearance variances and local structure changes commonly encountered in histological image registration scenarios. In this paper, for the first time, we propose a novel test-time adaptation method for histological image registration, aiming to improve the generalization ability of learning-based methods. Specifically, we design two operations, style guidance and shape guidance, for the test-time adaptation process. The former leverages style representations encoded by feature statistics to address the issue of appearance variances, while the latter incorporates shape representations encoded by HOG features to improve registration accuracy in regions with structural changes. Furthermore, we consider the continuity of the model during the test-time adaptation process. Different from the previous methods initialized by a given trained model, we introduce a smoothing strategy to leverage historical models for better generalization. We conduct experiments with several representative learning-based backbones on the public histological dataset, demonstrating the superior registration performance of our test-time adaptation method.
</div>

---

**[▲ 2024 New](/biomedical-imaging/2024-New){: style="color: rgb(191, 0, 0)"}**
**[▶ Segmentation](/biomedical-imaging/segmentation){: style="color: rgb(191, 0, 0)"}**
**[▶ Registration](/biomedical-imaging/registration){: style="color: rgb(191, 0, 0)"}**
**[▶ Restoration](/biomedical-imaging/restoration){: style="color: rgb(191, 0, 0)"}**
**[▶ Misc.](/biomedical-imaging/misc){: style="color: rgb(191, 0, 0)"}**
